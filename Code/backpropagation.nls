extensions [ table ]

globals 
[
  global-table
  
  num-examples-per-epoch
  epoch-error
  stop-criteria-met
]

to begin-train
  
  let dataset get-dataset
  
  train dataset
  
end

to-report get-dataset 
  
  let dataset table:make
  
  ;; get data from somewhere (???)
  
  report dataset
  
end

to train [ dataset ]
  
  set epoch-error 0
  set stop-criteria-met false
  
  let af ifelse-value ( activation-function = "hyperbolic-tangent" ) [ task hyperbolic-tangent-function ] [ task logistic-function ]
  let daf ifelse-value ( activation-function = "hyperbolic-tangent" ) [ task hyperbolic-tangent-function-derivative ] [ task logistic-function-derivative ] 
  
  let randomized-dataset ( shuffle table:to-list dataset )
  
  let epoch-dataset ( sublist randomized-dataset 0 num-examples-per-epoch )
  
  foreach epoch-dataset
  [
    ; pair of input-data and out-data is array
    ; input-data -> array
    ; out-data -> array
    let input-data ( item 0 ? )
    let out-data ( item 1 ? )
    
    activate-inputs input-data
    
    propagate af
    
    calc-output-nodes-error out-data
    
    back-propagate daf
  ]
  
  set epoch-error sum [ err ^ 2 ] of output-nodes
  
  tick
  
end

to activate-inputs [ input-data ]
  
  let i 0
  
  foreach sort-on [ who ] input-nodes
  [
    ask ? 
    [
      set activation ( item i input-data )
    ]
    
    set i i + 1 
  ] 
  
end

to propagate [ af ]
  
  foreach ( n-values num-hidden-layers [?] )
  [
    ask hidden-nodes with [ layer = ? ]
    [
      set activation (runresult af induced-local-field)
    ]
  ]
  
  ask output-nodes 
  [
    set activation (runresult af induced-local-field)
  ]
  
end

to-report induced-local-field
  
  report sum [ [ activation ] of end1 * weight ] of my-in-links
  
end


to calc-output-nodes-error [ expected-out ] 
  
  let i 0
  
  foreach sort-on [ who ] output-nodes 
  [
    ask ? 
    [
      set err expected-out - ( item i expected-out )
    ]
    
    set i ( i + 1 )
  ]
  
end

to back-propagate [ daf ]
  
  ask output-nodes 
  [
    set local-gradient err * ( runresult daf activation )
  ]
  
  foreach ( reverse n-values num-hidden-layers [?] )
  [
    ask hidden-nodes with [ layer = ? ]
    [
      set local-gradient ( runresult daf activation ) * sum [weight * [local-gradient] of end2] of my-out-links
    ]
  ]
  
  ask links with [ link-type = connection-link-type ]
  [
    set velocity momentum * velocity + learning-rate * [ activation ] of end1 * [ local-gradient ] of end2
    set weight weight + velocity
  ]
  
end